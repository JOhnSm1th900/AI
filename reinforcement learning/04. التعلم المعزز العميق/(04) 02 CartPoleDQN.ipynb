{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14232,"status":"ok","timestamp":1749720617929,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"},"user_tz":-60},"id":"fpPbM2qW5mSC","outputId":"1ae6f2a4-52c0-40b6-d1a6-eaac09bf8aae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"]}],"source":["!pip install gymnasium"]},{"cell_type":"code","source":[" # رفع الدرايف\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Zmj184gAepbF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# مكتبة البيئات\n","import gymnasium as gym\n","# مكتبة نمباي\n","import numpy as np\n","# مكتبة تنسر فلو للشبكات العصبية\n","import tensorflow as tf"],"metadata":{"id":"H5t-f6oGvev6","executionInfo":{"status":"ok","timestamp":1749720622081,"user_tz":-60,"elapsed":4146,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# إنشاء البيئة\n","env = gym.make('CartPole-v1')"],"metadata":{"id":"EXDuGnPqvesT","executionInfo":{"status":"ok","timestamp":1749720622150,"user_tz":-60,"elapsed":65,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# عدد الأفعال\n","num_actions = env.action_space.n\n","# عدد قيم الحالات\n","num_observations= env.observation_space.shape[0]\n"],"metadata":{"id":"N2FYgVGDvenT","executionInfo":{"status":"ok","timestamp":1749720622158,"user_tz":-60,"elapsed":4,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# شكل دخل الشبكة العصبية\n","input_shape = (num_observations,)"],"metadata":{"id":"z8TC-ZXhvekD","executionInfo":{"status":"ok","timestamp":1749720622196,"user_tz":-60,"elapsed":34,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# دالة مخصصة لتعريف نموذج شبكة عصبية\n","def create_dqn_model(input_shape, num_actions):\n","    # بناء شبكة عصبية تحتوي على طبقتين مخفيتين\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Input(shape=input_shape),  # طبقة الدخل\n","        tf.keras.layers.Dense(24, activation='relu'),  # الطبقة الأولى\n","        tf.keras.layers.Dense(24, activation='relu'),  # الطبقة الثانية\n","        # طبقة الخرج والتي عدد الخرج بعدد الأفعال\n","        tf.keras.layers.Dense(num_actions, activation='linear')\n","    ])\n","    return model"],"metadata":{"id":"F5-JIUwvvec7","executionInfo":{"status":"ok","timestamp":1749720622204,"user_tz":-60,"elapsed":6,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# الشبكة الرئيسية\n","dqn_agent = create_dqn_model(input_shape, num_actions)\n","\n","# الشبكة الهدف\n","target_network = create_dqn_model(input_shape, num_actions)"],"metadata":{"id":"cWjiyfbFwjW8","executionInfo":{"status":"ok","timestamp":1749720625103,"user_tz":-60,"elapsed":2902,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# المعاملات\n","learning_rate = 0.001  # معدل التعلم\n","discount_factor = 0.95  # معامل الخصم\n","epsilon = 1.0  # معدل الاستكشاف\n","epsilon_decay = 0.9955  # تخفيض الاستكشاف\n","min_epsilon = 0.1  # الحد الأدنى لإبسيلون\n","\n","batch_size = 64  # حجم الدفعة للتدريب"],"metadata":{"id":"g0MOcKYjwjSy","executionInfo":{"status":"ok","timestamp":1749720625161,"user_tz":-60,"elapsed":54,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from collections import deque\n","# الحجم الأعظمي لذاكرة التجارب\n","buffer_size = 1000000\n","# تهيئة ذاكرة التجارب\n","replay_buffer = deque(maxlen=buffer_size)"],"metadata":{"id":"Bf7ZUaN9wjPL","executionInfo":{"status":"ok","timestamp":1749720625168,"user_tz":-60,"elapsed":3,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# تعريف دالة الخسارة والمُحسن\n","# دالة الخسارة: الخطأ التربيعي المتوسط\n","loss_fn = tf.keras.losses.MeanSquaredError()\n","# المُحسن: آدم\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"],"metadata":{"id":"FMMdxI9qwjI7","executionInfo":{"status":"ok","timestamp":1749720625177,"user_tz":-60,"elapsed":5,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# دالة لتحديث أوزان الشبكة الهدف بشكل دوري\n","def update_target_network():\n","    target_network.set_weights(dqn_agent.get_weights())"],"metadata":{"id":"7EufQH0jwsdt","executionInfo":{"status":"ok","timestamp":1749720625181,"user_tz":-60,"elapsed":7,"user":{"displayName":"Rafika Cha","userId":"02763164898108319843"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# عدد الحلقات التدريبية\n","num_episodes = 2000\n","# بدء التدريب\n","for episode in range(num_episodes):\n","    # إعادة تهيئة البيئة وتهيئة الحالة\n","    state, info = env.reset()\n","    state = state[np.newaxis, :]  # إعادة تشكيل الحالة لتصبح (1, 4)\n","    done = False\n","    episode_reward = 0  # إجمالي المكافآت في الحلقة\n","\n","    while not done:\n","        # اختيار الفعل باستخدام سياسة إبسيلون\n","        if np.random.rand() < epsilon:\n","            # الاستكشاف بشكل عشوائي\n","            action = env.action_space.sample()\n","        else:\n","            # توقع القيم من النموذج\n","            q_values = dqn_agent(state)\n","             # اختيار الفعل الذي يحقق أكبر قيمة\n","            action = np.argmax(q_values.numpy())\n","\n","        # تنفيذ الفعل والحصول على الحالة التالية والمكافأة\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        next_state = next_state[np.newaxis, :]  # إعادة تشكيل الحالة التالية لتصبح (1, 4)\n","        episode_reward += reward  # إضافة المكافأة إلى إجمالي المكافأة في الحلقة\n","\n","        # تخزين التجربة في الذاكرة\n","        replay_buffer.append((state, action, reward, next_state, terminated or truncated))\n","\n","        # إذا كانت الذاكرة تحتوي على تجارب كافية، نبدأ في تدريب النموذج\n","        if len(replay_buffer) > batch_size:\n","            # اختيار دفعة من التجارب العشوائية\n","            batch = np.random.choice(len(replay_buffer), batch_size, replace=False)\n","\n","            # قوائم الحالات و الأفعال والمكافأت والحالات التالية والإتمام\n","            states_batch, actions_batch, rewards_batch, next_states_batch, done_flags_batch = zip(*[replay_buffer[idx] for idx in batch])\n","            # دمج القوائم في مصفوفات واحدة\n","            states_batch = np.vstack(states_batch)\n","            next_states_batch = np.vstack(next_states_batch)\n","\n","            # حساب قيم الخرج من الشبكة الرئيسية\n","            target_q_values = dqn_agent(states_batch).numpy()\n","            # حساب القيم المستهدفة باستخدام الشبكة الهدف\n","            next_q_values = target_network(next_states_batch).numpy()\n","            # إيجاد أكبر القيم\n","            max_next_q_values = np.max(next_q_values, axis=-1)\n","\n","            # استخدام صيغة بل مان لتحديث القيم\n","            for i, action in enumerate(actions_batch):\n","                target_q_values[i, action] = rewards_batch[i] + discount_factor * max_next_q_values[i] * (1 - done_flags_batch[i])\n","\n","            # حساب الخسارة\n","            with tf.GradientTape() as tape:\n","                current_q_values = dqn_agent(states_batch)\n","                loss = loss_fn(current_q_values, target_q_values)\n","\n","            # حساب التدرجات\n","            gradients = tape.gradient(loss, dqn_agent.trainable_variables)\n","            # استخدام المحسن لتحديث الأوزان\n","            optimizer.apply_gradients(zip(gradients, dqn_agent.trainable_variables))\n","\n","        # الانتقال إلى الحالة التالية\n","        state = next_state\n","\n","        # التحقق إذا كانت الحلقة قد انتهت\n","        if terminated or truncated:\n","            done = True\n","\n","    # تقليل احتمال الاستكشاف مع مرور الوقت\n","    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n","\n","    # تحديث أوزان الشبكة الهدف كل 10 حلقات\n","    if (episode + 1) % 10 == 0:\n","        update_target_network()\n","\n","     # طباعة تقدم التدريب كل 100 حلقة\n","    if (episode + 1) % 100 == 0:\n","        print(f\"Episode {episode + 1}: Reward = {episode_reward}, Epsilon = {epsilon:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnsN-MACwsaL","outputId":"22f21864-653d-4e1e-9c42-467edd2f7d42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 100: Reward = 28.0, Epsilon = 0.637\n","Episode 200: Reward = 316.0, Epsilon = 0.406\n","Episode 300: Reward = 42.0, Epsilon = 0.258\n"]}]},{"cell_type":"code","source":["# مسار مجلد العمل\n","working_folder='/content/drive/MyDrive/RLModels/'\n","# مسار النموذج\n","model_path = working_folder + \"cartpole_dqn_model.h5\""],"metadata":{"id":"OWv3DfhFzzAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifH8oz5aZBYj"},"outputs":[],"source":["# حفظ النموذج المدرب\n","dqn_agent.save(model_path)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}