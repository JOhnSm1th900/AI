{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"j5vOVmvFiuKJ"},"outputs":[],"source":["# استيراد المكتبات اللازمة\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# تعريف البيئة\n","n_rows = 6  # عدد الصفوف في الشبكة\n","n_cols = 6  # عدد الأعمدة في الشبكة\n","n_states = n_rows * n_cols  # إجمالي عدد الحالات في الشبكة"],"metadata":{"id":"7j_ZGQP3IO4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["goal_state = n_rows*n_cols-1  # حالة الهدف (الزاوية السفلى اليمنى)"],"metadata":{"id":"JtRY8DagLNXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_actions = 5  # عدد الأفعال الممكنة (أعلى، أسفل، يسار، يمين)"],"metadata":{"id":"TpsH8JRsLO99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# تهيئة جدول Q بقيم صفرية\n","Q_table = np.zeros((n_states, n_actions))\n","# تعريف المعاملات\n","learning_rate = 0.8  # معدل التعلم\n","discount_factor = 0.95  # عامل الخصم للمكافآت المستقبلية\n","exploration_prob = 0.2  # احتمال الاستكشاف\n","epochs = 100  # عدد حلقات التدريب"],"metadata":{"id":"Md5W13foLB_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# دالة لتحويل حالة معينة إلى إحداثيات (صف، عمود) في الشبكة\n","def state_to_position(state):\n","    return (state // n_cols, state % n_cols)"],"metadata":{"id":"UYWnco_LO-L7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# دالة لتحويل إحداثيات (صف، عمود) إلى حالة معينة\n","def position_to_state(row, col):\n","    return row * n_cols + col"],"metadata":{"id":"xHWTELwYQqzW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# دالة للحصول على الحالة التالية بناءً على الحالة الحالية والفعل\n","def get_next_state(current_state, action):\n","    row, col = state_to_position(current_state)\n","\n","    # تحديد الحركة بناءً على الفعل المختار\n","    if action == 0:  # الحركة إلى الأعلى\n","        row -= 1\n","    elif action == 1:  # الحركة إلى الأسفل\n","        row += 1\n","    elif action == 2:  # الحركة إلى اليسار\n","        col -= 1\n","    elif action == 3:  # الحركة إلى اليمين\n","        col += 1\n","\n","    # التأكد من بقاء العميل ضمن حدود الشبكة\n","    row = max(0, min(row, n_rows - 1))\n","    col = max(0, min(col, n_cols - 1))\n","    # الحالة التالية\n","    next_state = position_to_state(row, col)\n","    return next_state"],"metadata":{"id":"dVw-3JeOQsnf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# خوارزمية Q-learning\n","for epoch in range(epochs):\n","    current_state = np.random.randint(0, n_states-1)  # البداية من حالة عشوائية\n","    # استمر في التحرك حتى الوصول إلى الهدف\n","    while current_state != goal_state:\n","        # اختيار الفعل باستخدام استراتيجية epsilon-greedy\n","        if np.random.rand() < exploration_prob:\n","            action = np.random.randint(0, n_actions)  # استكشاف\n","        else:\n","            action = np.argmax(Q_table[current_state])  # استخدام المعرفة الحالية\n","\n","        # الحصول على الحالة التالية بناءً على الفعل المختار\n","        next_state = get_next_state(current_state, action)\n","\n","        # تعريف دالة المكافآت: عقوبة صغيرة لكل حركة، ومكافأة كبيرة للوصول إلى الهدف\n","        reward = -1  # عقوبة على كل حركة\n","        if next_state == goal_state:\n","            reward = 100  # مكافأة للوصول إلى الهدف\n","\n","        # تحديث قيمة Q باستخدام قاعدة التحديث الخاصة بـ Q-learning\n","        Q_table[current_state, action] = (1 - learning_rate) * Q_table[current_state, action] + \\\n","                learning_rate * (reward + discount_factor * np.max(Q_table[next_state]))\n","\n","\n","        current_state = next_state  # الانتقال إلى الحالة التالية\n","\n","# بعد التدريب، يمثل جدول Q القيم التي تم تعلمها\n","print(\"ََQ Table:\")\n","print(Q_table)"],"metadata":{"id":"Qi9yy5p7WPnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# دالة لإيجاد أقصر مسار\n","def find_path(Q_table, start_state, goal_state):\n","    current_state = start_state\n","    path = [current_state]  # قائمة لتخزين الحالات في المسار\n","\n","    while current_state != goal_state:  # استمر في البحث حتى الوصول إلى الهدف\n","        # اختيار أفضل فعل\n","        action = np.argmax(Q_table[current_state])  # الفعل الذي يحتوي على أعلى قيمة\n","\n","        # الحصول على الحالة التالية بناءً على الفعل المختار\n","        next_state = get_next_state(current_state, action)\n","\n","        # إضافة الحالة التالية إلى المسار\n","        path.append(next_state)\n","\n","        # تحديث الحالة الحالية\n","        current_state = next_state\n","\n","        # التوقف إذا تم الوصول إلى الهدف\n","        if current_state == goal_state:\n","            break\n","\n","    return path\n"],"metadata":{"id":"LMFDYZb6Sz4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Long Time\n","start_state = 0  # حالة البداية (الزاوية العليا اليسرى)\n","# إيجاد المسار من حالة البداية إلى حالة الهدف\n","path = find_path(Q_table, start_state, goal_state)\n","print(\"\\nPath from {} to goal {}:\".format(start_state, goal_state))\n","print(path)"],"metadata":{"id":"imIqlh2zx_Tv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# دالة لرسم الشبكة وعرض المسار\n","def plot_grid(n_rows, n_cols, path, start_state, goal_state):\n","\n","    # رسم الشبكة\n","    fig, ax = plt.subplots(figsize=(6, 6))\n","    # إنشاء شبكة\n","    ax.set_xticks(np.arange(n_cols + 1) - 0.5, minor=True)\n","    ax.set_yticks(np.arange(n_rows + 1) - 0.5, minor=True)\n","    ax.grid( which='minor',color='black', linestyle='-', linewidth=2)\n","\n","    # كتابة الحالات\n","    for i in range(n_rows):\n","        for j in range(n_cols):\n","            ax.text(j, i, str(i * n_cols + j), ha='center', va='center', fontsize=12, color='black')\n","\n","    plt.gca().invert_yaxis()  # عكس محور Y ليتوافق مع الشبكة\n","\n","\n","    # رسم المسار كدوائر حمراء\n","    path_positions = [state_to_position(state) for state in path]\n","    for (row, col) in path_positions:\n","        ax.plot(col, row, 'ro', markersize=8)\n","\n","     # تحديد مواقع البداية والهدف\n","    start_row, start_col = state_to_position(start_state)\n","    goal_row, goal_col = state_to_position(goal_state)\n","    ax.plot(start_col, start_row, 'go', markersize=10, label=\"Start\")  # بداية\n","    ax.plot(goal_col, goal_row, 'bo', markersize=10, label=\"Goal\")  # هدف\n","\n","    # إضافة العناوين\n","    ax.set_title(\"Agent Path\")\n","    ax.set_xticks(np.arange(n_cols))\n","    ax.set_yticks(np.arange(n_rows))\n","    ax.set_xticklabels(np.arange(0, n_cols ))\n","    ax.set_yticklabels(np.arange(0, n_rows ))\n","    # مفتاح الرسم\n","    ax.legend()\n","\n","    plt.show()\n","\n","# رسم المسار الذي تعلمه العميل في الشبكة\n","plot_grid(n_rows, n_cols, path, start_state, goal_state)"],"metadata":{"id":"jPTGinkKx9EM"},"execution_count":null,"outputs":[]}]}